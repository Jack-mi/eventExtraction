{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert+K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import codecs\n",
    "from transformers import BertModel, BertTokenizer, BertConfig\n",
    "\n",
    "model_path = '/Users/liuxiaosu/PycharmProjects/basic_model/chinese_L-12_H-768_A-12/'\n",
    "config_path = model_path + 'bert_config.json'\n",
    "checkpoint_path = model_path + '/bert_model.ckpt'\n",
    "dict_path = model_path + 'vocab.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"data/train.csv\")\n",
    "col = train_data.columns.tolist()\n",
    "\n",
    "train_data.drop(['category', 'query2', 'label'], axis=1, inplace=True)\n",
    "train_data = (train_data.values)\n",
    "# 3-category\n",
    "data1 = train_data[:100].tolist()\n",
    "data2 = train_data[500:600].tolist()\n",
    "data3 = train_data[1200:1300].tolist()\n",
    "train_data = data1 + data2 + data3\n",
    "# print((train_data[1]))\n",
    "# print((train_data[1139])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "myconfig = BertConfig(\n",
    "    output_hidden_states = True, \n",
    "    vocab_size = 21128,\n",
    "    )\n",
    "model = BertModel.from_pretrained(model_path, config = myconfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customize tokenizer for Chinese encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_dict = {}\n",
    "with codecs.open(dict_path) as dictionary:\n",
    "    for word in dictionary:\n",
    "        token = word.strip()\n",
    "        token_dict[token] = len(token_dict)\n",
    "\n",
    "class OurTokenizer4Chinese(Tokenizer):\n",
    "    def _tokenize(self, text):\n",
    "        R = []\n",
    "        for c in text:\n",
    "            if c in self._token_dict:\n",
    "                R.append(c)\n",
    "            elif self._is_whitespace(c):\n",
    "                R.append('[unused1]')\n",
    "            else:\n",
    "                R.append('[UNK]')\n",
    "        return R\n",
    "\n",
    "tokenizer = OurTokenizer(token_dict)\n",
    "tokenizer.tokenize('今天天气不错')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract all the vectors of the texts and save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "text2vec = []\n",
    "cnt = 0\n",
    "for text in train_data:\n",
    "    input_ids = torch.tensor(tokenizer.encode(text[0], add_special_tokens=True)).unsqueeze(0)\n",
    "    outputs = model(input_ids)\n",
    "    last_hidden_state = outputs[0]\n",
    "    cls = last_hidden_state[0][0]\n",
    "    text2vec.append(cls)\n",
    "    cnt += 1\n",
    "print(type(cls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, vec in enumerate(text2vec):\n",
    "    text2vec[i] = vec.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveFile(filename):\n",
    "    with open(filename, 'a') as f:\n",
    "        for vec in text2vec:\n",
    "            f.write(str(vec))\n",
    "            f.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveFile('textvec.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use kmeans to classify the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clustering documents ...\n",
      "Initialization complete\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 0, inertia 142.55057691211306\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 1, inertia 142.55057691211306\n",
      "center shift 0.000000e+00 within tolerance 1.264244e-04\n",
      "Initialization complete\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 0, inertia 144.9035300554161\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 1, inertia 144.9035300554161\n",
      "center shift 0.000000e+00 within tolerance 1.264244e-04\n",
      "Initialization complete\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 0, inertia 174.11934129246112\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 1, inertia 174.11934129246112\n",
      "center shift 0.000000e+00 within tolerance 1.264244e-04\n",
      "kmean: k = 3\n"
     ]
    }
   ],
   "source": [
    "print(\"clustering documents ...\")\n",
    "train_x = text2vec\n",
    "km = KMeans(n_clusters = 90,\n",
    "               max_iter = 1000,\n",
    "               tol = 0.001,\n",
    "               verbose = 1,\n",
    "               n_init = 3)\n",
    "km.fit(train_x)\n",
    "print(\"kmean: k = {}\".format(3, int(km.inertia_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = metrics.silhouette_score(X=train_x, labels=clusters)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
